{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Project 3</center>\n",
    "## <center>Probabilistic Search (and Destroy)</center>\n",
    "<p style='text-align: right;'> Github link: https://github.com/runfengxu/AI-probabilistic-search </p>\n",
    "<p style='text-align: right;'> Group Member: Wenhao Luo, Runfeng Xu, Bo Mao </p>\n",
    "\n",
    "\n",
    "## 1 --  A Stationary Target\n",
    "### 1) compute $P(Target\\ in\\;cell_i | Observation_t \\cap Failure\\; in\\; Cell_j)$\n",
    "\n",
    "\n",
    "+ __Beysian's Theorem__: $ {\\displaystyle P(Y\\;|\\;X,e)=\\frac{P(X\\;|\\;Y,e)P(Y\\;|\\;e)}{P(X\\;|\\;e)}}$<br/>\n",
    "\n",
    "\n",
    "+    In the project's circumstance: \n",
    "<br/><br/>\n",
    "     $\n",
    "     \\begin{aligned}\n",
    "     Y & = P(Target\\;in\\;Cell_i)\\\\\n",
    "     X & = P(Failure\\;in\\;Cell_j)\\\\\n",
    "     e & = Observation_t\n",
    "     \\end{aligned}\n",
    "     $\n",
    "<br/><br/><br/>\n",
    "+ ${\\displaystyle P(Target\\ in\\;cell_i | Observation_t \\cap Failure\\; in\\; Cell_j)=\\frac{P(Failure\\;in\\;Cell_j|Target\\;in\\;Cell_i,Observation_t)P(Target\\;in\\;Cell_i|\\;e)}{P(Failure\\;in\\;Cell_j|Observation_t)}\n",
    "=\\frac{P(Failure\\;in\\;Cell_j|Target\\;in\\;Cell_i)P(Target\\;in\\;Cell_i|\\;e)}{P(Failure\\;in\\;Cell_j|Observation_t)}}$\n",
    "     \n",
    "     \n",
    "+ Numeritor:${\\displaystyle P(Failure\\;in\\;Cell_j|Target\\;in\\;Cell_i)P(Target\\;in\\;Cell_i|\\;e)=P(False\\_Negative\\;for\\ Cell_i)P(Target\\;in\\;Cell_i|\\;e)}$\n",
    "+ Denominator:${\\displaystyle P(Failure\\;in\\;Cell_j|Observation_t)}$ can be expanded by __law of total probability__ <br/><br/>\n",
    "as :${ P(Failure\\;in\\;Cell_j|Observation_t,Not\\;in\\;Cell_j)P(Not\\;in\\;Cell_j|Observation_t)+P(Failure\\;in\\;Cell_j|Observation_t,In\\;Cell_j)P(In\\;Cell_j|Observation_t)}$<br/><br/>${\\displaystyle =P(Not\\;in\\;Cell_j|Observation_t)+P(False\\_Negative)P(Targer\\;In\\;Cell_j|Observation_t)}$ \n",
    "    \n",
    "     \n",
    "     \n",
    "- In the deductive equation:<br/>\n",
    "     ${ P(Failure\\;in\\;Cell_j\\;|\\;Target\\;in\\;Cell_j)=\\begin{cases}\n",
    "  0.1 &if\\;Cell_j\\;is\\;flat \\\\\\\\\n",
    "  0.3 &if\\;Cell_j\\;is\\;hilly \\\\\\\\\n",
    "  0.7 &if\\;Cell_j\\;is\\;forested \\\\\\\\\n",
    "  0.9 &if\\;Cell_j\\;is\\;caves\n",
    "\\end{cases}}$<br/>\n",
    "     which is unchanged because it is signed with 4 different terrian type at the beginning.\n",
    "     ${\\displaystyle observation_t)}$ is the possibility stored in our belief states at time t, it is updated after every search until the target is found and break the loop.\n",
    "\n",
    "### 2).find the probability that the target will be found in cell i if it is searched:\n",
    "+    $P(Target\\;found\\;in\\;Cell_i\\;|\\;Observation_t)$\n",
    "     $$\n",
    "     \\begin{align}\n",
    "     &= P(Target\\;found in\\;Cell_i\\;|\\;Observation_t \\cap Target\\;in\\;Cell_i) * P(Target\\;in\\;Cell_i\\;|\\;Observation_t) + P(Target\\;found\\;in\\;Cell_i\\;|\\;Target\\;not\\;in\\;Cell_i\\cap Observation_t) * P(Target\\;not\\;in\\;Cell_i\\;|\\;Ovservation_t)\\\\\n",
    "     &= P(Target\\;found\\;in\\;Cell_i\\;|\\;Observation_t \\cap Target\\;in\\;Cell_i) * P(Target\\;in\\;Cell_i\\;|\\;Observation_t)\\\\\n",
    "     &=(1-CurrentP(terrian's type)) * P(Target\\;in\\;Cell_i\\;|\\;Observation_t)\n",
    "     \\end{align}\n",
    "     $$\n",
    "     For this fomula, with the initial probability:<br/>\n",
    "     $P(Target\\;in\\;Cell_i\\;|\\;Observation_0) = \\frac{1}{2500}$\n",
    "     \n",
    "### 3).comparing rule1 and rule2:\n",
    "+    Based on the previous two question's analysis, this question is becoming quite straight forward since we can just use the part (1)&(2) answer to solve the rule1 and the rule2 only re-compute the probability by multiplying the probability in belief with the terrian type probability.\n",
    "\n",
    "+    For __rule1__, we have the code like:\n",
    "         point=np.where(self.belief_state == np.max(self.belief_state))\n",
    "     we just generally return an point where the probability in the belief is the maximum. The same as the requirement of rule1 which asks us to search teh cell with the highest probability of the containing target, which is choosing the highest probability from the belief set. \n",
    "+    For this rule, we run 50 times, and below is the data we collected.\n",
    "    \n",
    "| Terrian type | Rule1 AVG | Rule2 AVG |\n",
    "|:---:|:---:|:---:|\n",
    "|     Flat     | 2070.43| 515.538|\n",
    "|     Hill     | 3849.530612244898 | 1753.877551020408|\n",
    "|    Forest    |6375.34693877551 | 4974.897959183673|\n",
    "|     Cave     | 6218.142857142857|8716.959183673469 |\n",
    "| Total AVG for different rule|4628.36260204 |3990.31817347 |\n",
    "     \n",
    "     \n",
    "     \n",
    "+    For __rule2__, we have the code like:\n",
    "         matrix=np.zeros((self.dim,self.dim))\n",
    "         for i in range(self.dim):\n",
    "             for j in range(self.dim):\n",
    "                 p = self.belief_state[i][j]\n",
    "                 tp=map[i][j]\n",
    "                 if tp==1:\n",
    "                     matrix[i][j]=0.9*p\n",
    "                 elif tp==2:\n",
    "                     matrix[i][j]=0.7*p\n",
    "                 elif tp==3:\n",
    "                     matrix[i][j] = 0.3 * p\n",
    "                 elif tp==4:\n",
    "                     matrix[i][j] = 0.1 * p\n",
    "          point=np.where(matrix==np.max(matrix))\n",
    "     We first generated an all zero matrix fill with all position with '0'. Then retrieve probability from belief set; finally multiple them with corresponding terrian type probability.\n",
    "\n",
    "+    For this rule, we run 50 times, and the collected data is in the above table.\n",
    "\n",
    "+    Firstly, we find that for the terrian of Cave, each time, it passed more than 10,000 searches. So, we manually set the maximum searching steps as 10,000. Here is the code we wrote:\n",
    "          while ((not result) and (num<10000)):\n",
    "              tp=map[x][y]\n",
    "              Ai.update_state(x,y,tp)\n",
    "              x,y=Ai.rule2_sec4(x,y,map)\n",
    "              result = Ai.search_cell(x,y,a,b,map)\n",
    "              num=num+1\n",
    "\n",
    "+    To compare this two method, we can easily find out that the average of __rule2__ performs worse than __rule1__. The reason is that the rule2 is strictly searching target in an order which is: Flat --> Hill --> Forest --> Cave. Which may takes a large amount of time if the target is in Forest or Cave. However, for the __rule1__, the method keeps searching target in the highest probability of the belief set, which may jump into another terrian type frequently each time. Therefore, the __rule1__ performs better than __rule2__ overall. \n",
    "\n",
    "+    Additionally, if the target is in Flat or Hill, the rule2 may performs better than rule1; vice versa, if the target is in Forest or Cave especially, the rule1 is much better. However, even though it seems that the rule2 will perform better when the target is in Flat or Hill.<br/>$P(Target\\;not\\;found\\;in\\;Cell_i\\;|\\;Target\\;in\\;Cell_i)$ lowers the upper bound performance of rule2, since the probability of given target in Flat or Hill and not find it in that cell is too small, which is (0.1 , 0.3) respectively.\n",
    "\n",
    "### 4).search each constitute a single 'action':\n",
    "+    In this problem, we create a way to calculate it as:\n",
    "$$\n",
    "priority = \\frac{current_P}{manhattandist}\n",
    "$$\n",
    "+    The $P(current_p)$ means the cell's current probability if the belief set. \n",
    "+    For this 'priority',  we have the code like:\n",
    "         matrix = np.zeros((self.dim, self.dim))\n",
    "         for i in range(self.dim):\n",
    "             for j in range(self.dim):\n",
    "                 p = self.belief_state[i][j]\n",
    "                 matrix[i][j] = p/manhattan(i,j,a,b)\n",
    "         point = np.where(matrix == np.max(matrix))\n",
    "+    In this expression, the value of priority is greater means AI agent will search that cell before searching other cells. \n",
    "+    Additionally, we choose each unit movement as 1 action.\n",
    "+    Furthermore, we collected data to generate a table as below:\n",
    "\n",
    "| Terrian type |  Rule1 based on manhattan |Rule2 based on manhattan|\n",
    "|:---:|:---:|:---:|\n",
    "|     Flat     | | |\n",
    "|     Hill     | | | \n",
    "|    Forest    | | | \n",
    "|     Cave     | | |\n",
    "| Total AVG for different terrian type|\n",
    "     \n",
    "### 5).old joke inspiration:\n",
    "+    In this joke, the target we are finding in the 50*50 grid is the drunk man's lost key. Furthermore, the drunk man searched his keys under the light, however, he is not sure about where he lost keys. So, in this senario, the drunk man is searching his keys following the rule2 in part(3), that he keeps finding the keys at the place where it reasonablly find much easier. Comparing with rule2 in part(3), the AI keeps searching the target from Flat to Cave terrian because the Flat has highest possibility finding the key because  $P(Found\\;in\\;Cell_i\\;|\\;Target\\;in\\;Cell_i)$ is highest with P=0.9 in all different terrian.\n",
    "+    But, in this joke, we are not been given that the place affects the probability of finding keys. However, what we know is that $P(found\\;in\\;park\\;|\\;keys\\;in\\; there) > P(found\\;in\\;street\\;|\\;keys\\;in\\;there)$, so we assume<br/><br/>\n",
    "$$P(lost\\;in\\;park) = 0.8$$\n",
    "$$P(lost\\;in\\;street) = 0.2$$\n",
    "Therefore, it almost likely that we can use the results from Rule1 of Hill and Rule2 of Forest to compare and then give the suggestion to the drunk man where to find the key is better. From the data, we can say that finding in the street with light is better. However, the factor of how the drunk man remember the place he lost keys is very important here, it related to the<br/><br/> \n",
    "$$P(lost\\;in\\;park) = 0.??$$\n",
    "$$P(lost\\;in\\;street) = 0.??$$\n",
    "Considering that, if the probability of there two place is (0.5, 0.5). Then, we can confident say that finding the keys in the street is always better.\n",
    "\n",
    "\n",
    "\n",
    "## Section_2: A Moving Target\n",
    "### <font face=\"Times New Roman\"><font size=\"5\">*Issue(i): Re-do question 4) above in this new environment with the moving target and extra information.*</font>\n",
    "### <font face=\"Times New Roman\"><font size=\"5\">Sol: </font>\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">By using surveillance reports we can easily reduce the scale of search areas.</font>\n",
    "\n",
    "#### <font face=\"Times New Roman\"><font size=\"4\">Stage 1: randome search a cell and get report from the target.</font>\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">Initially, we will randomly start a first search on the terrain map. That is point to the right cell with the probability of 1/2500. Which is a tiny value even waive to consider the situation of false negative. After this search, we will update the probability (based on the ***rule1*** or ***rule2***) for each cell.</font>\n",
    "\n",
    "#### <font face=\"Times New Roman\"><font size=\"4\">Stage 2: reduce the searching area.</font>\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">However, after first search, we will reduce the scale of search area into two two type of terrains for the worst case and only one type of terrain cell for a optimal case. </font>\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">In our algorithm, we use a dictionary to collect those reduced cells and their probabilities. For example, if the report we get from target is ***(Flat, Hill)***, we will collect flat cells and hill cells into one dictionary and then normalize them: </font>\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">The format for this dictionary is $Dict = \\{ Cell:Probability \\}$</font>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Flat &= \\{ (0,1): 0.1, (2,2): 0.1, (3,0):0.05 \\} \\\\\\\\\n",
    "Hill &= \\{ (2,1): 0.05, (0,0): 0.15, (4,2): 0.05 \\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">Before normalization, we will delete the cells that are not adjacent with another type of cell. In this case, flat cell (3, 0) is not adjacent with any cells in hill, so we delete it. Hill cell (4, 2) is also in the same case.</font>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Filtered\\_Flat &= \\{ (0,1): 0.1, (2,2): 0.1\\} \\\\\\\\\n",
    "Filtered\\_Hill &= \\{ (2,1): 0.05, (0,0): 0.15\\}\\\\\\\\\n",
    "=> ProbMap &= \\{ (0,1): 0.1, (2,2): 0.2, (2,1): 0.05, (0,0): 0.15 \\} \\\\\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">Then, we normalize it.</font>\n",
    "\n",
    "$$\n",
    "=> Normalized\\_ProbMap = \\{ (0,1): 0.25, (2,2): 0.25, (2,1): 0.125, (0,0): 0.375\\}\n",
    "$$\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">If the types in report are the same, it can also be modified in the same way.</font>\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">then we can choice the cell with high priority for the next search.</font>\n",
    "\n",
    "#### <font face=\"Times New Roman\"><font size=\"4\">Stage 3: Further reducing the search scale.</font>\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">If the report is always same as the first report, then the searching scale will not change.</font>\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">However, when the report changed, for example we get a report ***(Hill, Forest)***, we can easily reduce the searching scale to the ***forest cells***. The intersection of the report sets is the previous step and the last is the current type of cell that target exist.</font>\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">In this case, we can use the knowledge base, to expand it, for example: Initially, we have:</font>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "First\\_Filtered\\_Flat &= \\{ (0,1): 0.1, (2,2): 0.1\\} \\\\\\\\\n",
    "First\\_Filtered\\_Hill &= \\{ (2,1): 0.05, (0,0): 0.15\\}\\\\\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">Now we get the report: ***(Forest, Hill)***, since we already know all the Forest cells. Initially, their probabilities are zero according to the previous information, suppose the dictionary for forest cells is:</font>\n",
    "\n",
    "$$\n",
    "Forest = \\{ (3,1): 0, (1,0): 0, (1,1): 0, (2, 0): 0  \\}\n",
    "$$\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">Then, using the filtered hill cells to filter forest cells and initialize the probability of each cell.</font>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "First\\_Filtered\\_Hill &= \\{ (2,1): 0.125, (0,0): 0.375\\}\\\\\\\\\n",
    "Forest &= \\{ (3,1): 0, (1,0): 0, (4,0): 0, (2, 0): 0  \\} \\\\\\\\\n",
    "\\because (2, 1) &=> \\{ (3,1), (2,0) \\} \\\\\\\\\n",
    "\\because (0, 0) &=> \\{ (1, 0) \\} \\\\\\\\\n",
    "Filtered\\_Forest &= \\{ (3,1): 0.125 * (1/2), (2,0): 0.125 * (1/2), (1,0): 0.375 \\} \\\\\\\\\n",
    "Filtered\\_Forest &= \\{ (3,1): 0.0625, (2,0): 0.0625, (1,0): 0.375 \\} \\\\\\\\\n",
    "Normalized\\_Filtered\\_Forest &= \\{ (3,1): 0.125, (2,0): 0.125, (1,0): 0.75 \\} \\\\\\\\\n",
    "Normalized\\_ProbMap &= Normalized\\_Filtered\\_Forest\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">As showed above, for each new filtered cell, the probability it inherited is the probability of the previous cell that can expand to it divided by the total number of branches. For example, $A \\in Type1$, $C \\in Type1$ and $B \\in Type2$, if we can confirm that target is moving from Type1 to Type2 and both A and C are adjacent to B, the number of branches from cell A to Type2 is ***numA***, the number of branches from cell C to Type2 is ***numC***, then the probability of B that inherited is: </font>\n",
    "\n",
    "$$\n",
    "P(B) = P(A)/numA +P(C)/numC\n",
    "$$\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">After all the iteration, normalize it and keep the search based on rule1 or rule2.</font>\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">If the report we get is ***(Flat, Flat)***, we can also reduce the search scale follow the same way, and this iteration will keep going until we find it</font>\n",
    "\n",
    "+  <font face=\"Times New Roman\"><font size = \"4\">Table below shows the comparison of our data, we can find that the cost of action is reduced largely with the report information from the target:</font>\n",
    "\n",
    "|    |<font size = \"4\">Cost with stationary target</font>|<font size = \"4\">Cost with Moving target's report</font>|\n",
    "|:---:|:---:|:---:|\n",
    "|<font size = \"4\">Rule 1</font>|<font size = \"4\"></font>|<font size = \"4\">2061.7</font>|\n",
    "|<font size = \"4\">Rule 2</font>|<font size = \"4\"></font>|<font size = \"4\">2844.16</font>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
